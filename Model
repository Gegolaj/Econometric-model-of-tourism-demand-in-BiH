import numpy as np
import pandas as pd
import csv
import matplotlib as plt 
from matplotlib.pyplot import figure
import seaborn as sns
import matplotlib.pyplot as plt
import math
from sklearn.linear_model import LinearRegression
import statsmodels.api as sm
import statsmodels.stats.diagnostic as dg
from statsmodels.tsa.stattools import adfuller
import statsmodels.stats.outliers_influence as oi
from statsmodels.stats.diagnostic import linear_reset
import scipy.stats as stats
from statsmodels.stats.diagnostic import het_white
from statsmodels.stats.outliers_influence import variance_inflation_factor

 # We start with importing modules tahat we will need

1. Introduction

USA = pd.read_excel(r'C:\Users\intel\Desktop\diplomski.dohodak.xlsx',sheet_name="Data_USA")
GER = pd.read_excel(r'C:\Users\intel\Desktop\diplomski.dohodak.xlsx',sheet_name="Data_GER")
CHN = pd.read_excel(r'C:\Users\intel\Desktop\diplomski.dohodak.xlsx',sheet_name="Data_CHN")

 # USA: In this data set are information about United States of America
 # GER: In this data set are information about Germany
 # CHN: In this data set are information about China

 # Tourism as an industrial branch of the economy is based on the need to temporarily change the place of residence of tourists, 
 # which includes the element of travel and the element of temporary residence in another place.
 # Temporary change of tourist stay means the arrival and gathering of tourists in certain places so that these places become "tourist destinations". 
 # Tourist destinations are places where tourists meet their needs to change their place of residence, the need to meet new cultures, the need to explore, etc.
 # The most important economic aspect of tourism is the consumption of tourists in the places they visit. 
 # The spending of money by tourists has certain economic effects on the economy of the country that tourists visit. The economic effects of tourism are:

 - the impact of tourism on gross domestic product and national income,
 - impact on the country's balance of payments,
 - impact on investment activity and investment structure,
 - multiplier effect on the economy,
 - impact on population employment.
 
 # The subject of our research is modeling the demand for tourism in BiH (Bosnia and Herzegovina)
 # By formulating a model of tourism demand, we will try to determine:
 - statistically significant model of tourism demand,
 - independent variables that explain the model,
 - coefficients with model parameters,
 - determine whether there are independent variables in the model that are not statistically significant.

 # By defining the model of tourism demand, we want to achieve the following goals:
 - determine the impact of independent variables on the demand for tourism in BiH,
 -provide information to the government in order to make tourism policy decisions more efficient and effective.

2. Methodology
 2.1. Model selection

 # Modified model of tourism demand that we will model:
 - DTij = f (Yj, RPij)
 # The variables in the model for which we have data and which we will use for modeling are:
 - the number of tourists from country j in BiH given in quarterly intervals,
 - income measured as disposable income of tourists from the country of origin j (Yj),
 - the ratio of relative prices of BiH and the country of origin of tourists j (Rpij).

 # Given the data on the largest consumers in the field of tourism, the countries that I decided to include in the model of tourism demand are:
 - United States (USA),
 - The Republic of Germany,
 - People's Republic of China.
 
 # We decided on the United States due to its geographical distance from BiH and the fact that the United States is the most developed country, 
 # and the country with the strongest political and cultural influence in the world.
 
 # Germany was chosen because of the three selected countries, the closest to BiH in geographical distance, 
 # and the most economically developed country on the European continent.

 # We chose China because the number of tourists from this country to Bosnia and Herzegovina is increasing every year. In addition, of the selected countries, 
 # China is the country with the largest population in the world and the second largest economy in the world.

 2.2.Identification of model variables

 # The variables that we decided to evaluate the model of tourism demand in Bosnia and Herzegovina based on the theory and availability of data are variables:
 - number of tourists
 - income
 - relevant prices
 
 # The number of tourists in our model represents the number of tourists in our country in one quarter.
 # The variable income in our model represents the income of tourists from the country of origin measured as the disposable personal income of the population.
 # The relative price variable represents the ratio of the consumer price index of the visitor's country to the home country.

 2.3.Graphic representation of data

figure(figsize=(8, 6), dpi=100)
plt.plot(USA["Period"],USA["Bt.USA"],marker='o')
plt.title('Number of tourist by quartal data USA')
plt.xlabel('Bt.USA')
plt.ylabel('Period')
plt.show()
 # Code to make line chart to represent data USA 

figure(figsize=(8, 6), dpi=100)
plt.plot(GER["Period"],GER["Bt.GER"],marker='o')
plt.title('NUmber of tourist by quartal data GER')
plt.xlabel('Bt.GER')
plt.ylabel('Period')
plt.show()
 # Code to make line chart to represent data Germany 

figure(figsize=(8, 6), dpi=100)
plt.plot(CHN["Period"],CHN["Br.CHN"],marker='o')
plt.title('Number of tourist by quartal data CHN')
plt.xlabel('Bt.CHN')
plt.ylabel('Period')
plt.show()
 # Code to make line chart to represent data China 

3. Descriptive statistics
 3.1.Descriptive statistics of variables of interest

 # Descriptive statistics serve us to organize, describe and analyze the collected data.

print(USA.info()) 
 # Code to gain information about data USA
    Column   Non-Null Count  Dtype  
---  ------   --------------  -----  
 0   Period   38 non-null     object 
 1   Bt.USA   38 non-null     int64  
 2   RP(USA)  38 non-null     float64
 3   DI(USA)  38 non-null     float64 
 # We see that for data USA we have 38 data entries.

print(USA.isnull().sum())
Period     0
Bt.USA     0
RP(USA)    0
DI(USA)    0
 # With this code we are looking does our data have missing values. We see that for data USA we have no missing values.

print(GER.info())
 # Code to gain information about data Germany
    Column   Non-Null Count  Dtype  
---  ------   --------------  -----  
 0   Period   38 non-null     object 
 1   Bt.GER   38 non-null     int64  
 2   RP(GER)  38 non-null     float64
 3   DI(GER)  38 non-null     float64
 # We see that for data Germany we have 38 data entries.

print(GER.isnull().sum())
Period     0
Bt.GER     0
RP(GER)    0
DI(GER)    0
 # With this code we are looking does our data have missing values. We see that for data Germany we have no missing values.

print(CHN.info())
 # Code to gain information about data China
    Column   Non-Null Count  Dtype  
---  ------   --------------  -----  
 0   Period   38 non-null     object 
 1   Br.CHN   38 non-null     int64  
 2   RP(CHN)  38 non-null     float64
 3   DI(CHN)  38 non-null     float64
 # We see that for data Germany we have 38 data entries.

print(CHN.isnull().sum())
Period     0
Br.CHN     0
RP(CHN)    0
DI(CHN)    0
 # With this code we are looking does our data have missing values. We see that for data China we have no missing values.


USA.describe().round(3) # Code to do data summary, also we have rounded the decimal points to 3 digits
GER.describe().round(3) # Code to do data summary, also we have rounded the decimal points to 3 digits
CHN.describe().round(3) # Code to do data summary, also we have rounded the decimal points to 3 digits


	Bt.USA	   RP(USA)	DI(USA)		        Bt.GER	  RP(GER)    DI(GER)		Br.CHN	  RP(CHN)  DI(CHN)
count	38	   38	        38	        count	38	  38	     38	       count	38	  38	   38
mean	4112.816   0.686	11961.996	mean	6905.868  1.94	     447.776   mean	4737.316  0.244	   3968.02
std	2804.895   0.038	1263.085	std	4403.772  0.037	     32.558    std	7397.877  0.02	   905.039
min	1042	   0.618	9879.049	min	2342	  1.873	     395.9     min	98	  0.203	   2475.273
25%	1991.5	   0.661	11020.757	25%	3632.5	  1.909	     423.662   25%	622	  0.239	   3259.346
50%	3062.5	   0.683	11935.191	50%	6002.5	  1.933      444.06    50%	1750.5	  0.247	   3979.456
75%	5100.75	   0.709	12908.066	75%	7947.5	  1.971      474.558   75%	4902.25	  0.254	   4679.711
max	11366	   0.759	14285.796	max	20737	  2.002      508.3     max	36831	  0.282	   5487.895

# To save space and for better view we have put data for USA, Germany and China in one table.

4. Data analysis
 4.1.Choice of estimator

 # By data analysis we mean the use of statistical methods and procedures on a sample for statistical inference. 
 # For data analysis, we will use regression analysis, ie. least squares method (OLS).
 # To use the least squares method, certain estimator assumptions must be met:
 - linearity of model parameters,
 - random sample,
 - independence of the expected value of the random member,
 - lack of perfect correlation between variables,
 - the variance of the random member is constant (homoskedasticity),
 - normality of a random member.
 # If the assumptions from 1st to 4th are met, we say that the model is unbiased, and if all the assumptions are met, then OLS is the best unbiased linear estimator (BLUE).
 # One of the tests we will do is the Durbin-Watson test to check the existence of serial autocorrelation, and in case of the existence of autocorrelation, apply the methods of removing it.
 # Another test related to time series is the Dickey-Fuller test for checking stationarity, ie. the existence of a unit root.
 # After the time series tests, we will do other diagnostic tests:
 - Ramsey Reset test to check the correctness of the functional form of the model,
 - Breush-Pagan and White test to test for homosexuality,
 - Jarque-Bera test to examine the existence of residual normality.

 4.2.Regression analysis and model evaluation

 # One of the most commonly used models of tourism demand has the form:
           Qij = f (Yj, TCij, RPij, ERij, QFi, Tj, Aij, εij)
 
 # In this paper, due to lack of data, lack of data in quarterly reports, inability to conduct research to determine the preferences of tourists, 
 # a modified model of tourism demand with two independent variables is proposed, which takes the form:
           DTij = f (Yj, RPij)
 # The proposed model of tourism demand has a linear functional form in the form:
           DT = β0 + β1Yj + β2RPij + ε

 # Although the linear form of the tourism demand model is easier to use, the functional form more commonly used in research is the degree of functional form of the tourism demand model.
 # Emirical research has shown that the degree model has better performance compared to the linear model in terms of expected coefficient signs and statistical significance of coefficients.
 # Due to the above characteristics of the degree model of tourism demand, the theoretical model of tourism demand with the number of tourists as a dependent variable 
 # and income and relative price as an independent variable used in the work after the transformation of the degree model is:
          
           log number of tourists = β0 + β1log disposable income + β2log relative prices + εij

 # The obtained demand model is a log-log model, ie. constant elasticity model.

 # Before we continue with anlysis we must first transform data into log form

USA = USA.drop("Period",axis=1)
USA['log_Bt.USA'] = np.log(USA['Bt.USA'])
USA['log_RP(USA)'] = np.log(USA['RP(USA)'])
USA['log_DI(USA)'] = np.log(USA['DI(USA)'])
USA = USA.drop("Bt.USA",axis=1)
USA = USA.drop("RP(USA)",axis=1)
USA = USA.drop("DI(USA)",axis=1)
USA.head(2)

    log_Bt.USA	log_RP(USA)  log_DI(USA)
0   9.251866	-0.346997    9.567021
1   8.026824	-0.368342    9.556195
 # Code we use to drop "Period" feature. Rest of the code is transformation of data into natural logaritham for data USA.
 # In output we see that we have make the transformation

GER = GER.drop("Period",axis=1)
GER['log_Bt.GER'] = np.log(GER['Bt.GER'])
GER['log_RP(GER)'] = np.log(GER['RP(GER)'])
GER['log_DI(GER)'] = np.log(GER['DI(GER)'])
GER = GER.drop("Bt.GER",axis=1)
GER = GER.drop("RP(GER)",axis=1)
GER = GER.drop("DI(GER)",axis=1)
GER.head(2)

   log_Bt.GER	log_RP(GER)  log_DI(GER)
0  9.939675	0.658091     6.231072
1  8.347590	0.648411     6.224598
 # Same code we used to transform data of Germany

CHN = CHN.drop("Period",axis=1)
CHN['log_Br.GER'] = np.log(CHN['Br.CHN'])
CHN['log_RP(CHN)'] = np.log(CHN['RP(CHN)'])
CHN['log_DI(CHN)'] = np.log(CHN['DI(CHN)'])
CHN = CHN.drop("Br.CHN",axis=1)
CHN = CHN.drop("RP(CHN)",axis=1)
CHN = CHN.drop("DI(CHN)",axis=1)
CHN.head(2)

   log_Br.GER  log_RP(CHN)  log_DI(CHN)
0  10.514095   -1.355666    8.610300
1  8.990442    -1.361265    8.596397
 # Same code we used to transform data of China

4.2.1.Examination of the existence of serial correlation

 # Time series represent random variables Xt recorded at time t. One of the assumptions of the OLS estimator is the independence of the expected value of the random member. 
 # This means that there must be no correlation between the variables included in the model and the variables that ended up in the random member of the model. 
 # If this assumption is violated, ie. there is a correlation then there is a problem of autocorrelation in time series.
 # The consequences of the existence of autocorrelation are:
 - estimates of model parameters are unbiased but inefficient,
 - the assessment of the variance of a random member is biased,
 - R2 is not a valid indicator of regression quality,
 - t and F test results biased and ineffective.
 # If there is an autocorrelation, the methods we can use to correct the autocorrelation problem are:
 - check that autocorrelations are not the result of an incorrectly specified model,
 - apply another estimator such as the generalized least squares method,
 - apply the Cochrane-Orcut method,
 - apply Newey-West standard error corrections.
 # The test based on which we check the serial autocorrelation is the Breush-Godfrey test. The Breush-Godfrey test is not limited to first-order autocorrelation testing, 
 # which allows us to test higher-order autocorrelation, and the conclusion about accepting / rejecting hypotheses is based on the χ2 distribution.
 # The hypotheses of the Breush-Godfrey test are formed in the following way:
   - H0: ρ1 = ρ2 =. . . = ρp = 0 (no autocorrelation)
   - H1: ρ ≠ 0 (there is autocorrelation)
 # Autocorrelation tells us that event Xt is affected by an event that occurred at time Xt-k. The autocorrelation of the k-th order tells us how long it takes to go back in time. 
 # This return to the past is called a lag (shift) of the time series.
 # The choice of the displacement length is made on the basis of information criteria.
 # Since this is a quarterly data and a sample of 38 data, the information criterion we will use is the Schwarz information criterion (BIC).
 # The number of lag is calculated by repeating the model, each time with variables shifted by 1 so that we create a lag data set.
 # We repeat the procedure until we obtain the smallest value of BIC, indicating that number of shifts we used is our lag value.
 
USA

Y_USA = USA["log_Bt.USA"]
X_USA = USA.drop("log_Bt.USA",axis=1)
 # Creating a independing variable X_USA and depending variable Y_USA
X = sm.add_constant(X_USA)
lin_USA = sm.OLS(Y_USA,X).fit()
lin_USA.summary()
 # Creating a regression model  		

 # To check the autocorrelation we will do Breush-Godfrey test. In the test we will be changing 
 # the number of lag until we gain p-value that will reject null hypotheses.

print(dg.acorr_breusch_godfrey(lin_USA, nlags=2))
 # Code to do Breush-Godfrey test with number of lag = 2

χ2	  p-value    # Based on p-value we reject null hypotheses and conclude that we have
-24.6323  0.0000045  # serial autocorrelation in our model.

lin_USA_HAC= lin_USA.get_robustcov_results(cov_type='HAC',maxlags=2)
 # With this code we will make correction of standards errors with lag 2.
 # Standard Errors are heteroscedasticity and autocorrelation robust (HAC) using 2 lags.

Germany

Y_GER = GER["log_Bt.GER"]
X_GER = GER.drop("log_Bt.GER",axis=1)
 # Creating a independing variable X_GER and depending variable Y_GER
X = sm.add_constant(X_GER)
lin_GER = sm.OLS(Y_GER,X).fit()
lin_GER.summary()
 # Creating a regression model

 # To check the autocorrelation we will do Breush-Godfrey test. In the test we will be changing 
 # the number of lag until we gain p-value that will reject null hypotheses.

print(dg.acorr_breusch_godfrey(lin_GER, nlags=4))
 # Code to do Breush-Godfrey test with number of lag = 4

χ2	 p-value   # Based on p-value we reject null hypotheses and conclude that we have
27.6868  0.000014  # serial autocorrelation in our model.

lin_GER_HAC= lin_GER.get_robustcov_results(cov_type='HAC',maxlags=4)
 # With this code we will make correction of standards errors with lag 4.
 # Standard Errors are heteroscedasticity and autocorrelation robust (HAC) using 4 lags.

China

Y_CHN = CHN["log_Br.CHN"]
X_CHN = CHN.drop("log_Br.CHN",axis=1)
 # Creating a independing variable X_CHN and depending variable Y_CHN
X = sm.add_constant(X_CHN)
lin_CHN = sm.OLS(Y_CHN,X).fit()
lin_CHN.summary()
 # Creating a regression model

 To check the autocorrelation we will do Breush-Godfrey test. In the test we will be changing 
 # the number of lag until we gain p-value that will reject null hypotheses.

χ2	 p-value    # Based on p-value we reject null hypotheses and conclude that we have
28.8004	 0.0000086  # # serial autocorrelation in our model.

lin_CHN_HAC= lin_CHN.get_robustcov_results(cov_type='HAC',maxlags=4)
 # With this code we will make correction of standards errors with lag 4.
 # Standard Errors are heteroscedasticity and autocorrelation robust (HAC) using 4 lags.

4.2.2. Examining the existence of a singular root

 # By testing the existence of a unit root, we want to determine whether the time series is stationary.
 # We say that a time series is stationary if the following properties apply:
 - the mean value of Xt is constant over time E (Xt) = µ = const.
 - the variance Xt is constant over time v (Xt) = E (Xt-µ) 2 = const.
 - covariance is constant over time E [(Xt - μ) (Xt-p - μ)] = const.
 # If at least one of the listed properties is not valid for time series, then we say that the time series is non-stationary.
 # The term unit root is associated with nonstationary time series. 
 # The unit root is a number that tells us how many times a time series needs to be differentiated to become a stationary time series.
 # In case of non-stationarity, the methods we can use to correct the non-stationarity problem are:
 - include trend as an independent variable in the model,
 - differentiate data,
 - check the existence of cointegration between time series.
 # The application of the Dickey-Fuller test begins with the transformation of the auto regression model and the calculation of t-statistics to draw conclusions about the statistical significance of the test. 
 # The hypothesis we test using the Dickey-Fuller test is:
    H0: ß1 = 0
    H1: ß1 ˂ 0

USA

adfuller(USA["log_Bt.USA"],autolag="BIC") # Based on p-value we can not reject null hypothesis,meaning data is non-stationary
 ADF statistic	p-value
 -1.3048 	 0.6270 

adfuller(USA["log_RP(USA)"],autolag="BIC") # Based on p-value we can not reject null hypothesis,meaning data is non-stationary
 ADF statistic	p-value
 -3.3205 	 0.0140 

adfuller(USA["log_DI(USA)"],autolag="BIC") # Based on p-value we can not reject null hypothesis,meaning data is non-stationary
 ADF statistic	p-value
 -0.1360 	 0.9457 

 # We say that nonstationary variables are cointegrated if there is a degree in their motion that neutralizes individual nonstationarity.
 # We will test the existence of cointegration by calculating the model residues, and then we will test the residues with the Dickey-Fuller test.

resid = lin_USA_HAC.resid
adfuller(resid,autolag="BIC") # Based on p-value we can reject null hypothesis,meaning data is stationary
 ADF statistic	p-value
 -3.9150 	 0.0019 

Germany

adfuller(GER["log_Bt.GER"],autolag="BIC") # Based on p-value we can reject null hypothesis,meaning data is stationary
 ADF statistic	p-value
 -6.2909 	 0.0000 

adfuller(GER["log_RP(GER)"],autolag="BIC") # Based on p-value we can not reject null hypothesis,meaning data is non-stationary
 ADF statistic	p-value
 -1.1543 	 0.6929 

adfuller(GER["log_DI(GER)"],autolag="BIC") # Based on p-value we can not reject null hypothesis,meaning data is non-stationary
 ADF statistic	p-value
 -0.7210 	 0.8412 

 # We will test the existence of cointegration by calculating the model residues, and then we will test the residues with the Dickey-Fuller test.

resid = lin_GER_HAC.resid
adfuller(resid,autolag="BIC) # Based on p-value we can reject null hypothesis,meaning data is stationary
ADF statistic	p-value
 -7.247 	 0.000 

China

adfuller(CHN["log_Br.CHN"],autolag="BIC") # Based on p-value we can not reject null hypothesis,meaning data is non-stationary
 ADF statistic	p-value
 -0.2083 	 0.9375 

adfuller(CHN["log_RP(CHN)"],autolag="BIC") # Based on p-value we can not reject null hypothesis,meaning data is non-stationary
 ADF statistic	p-value
 -1.1581 	 0.6913 

adfuller(CHN["log_DI(CHN)"],autolag="BIC") # Based on p-value we can not reject null hypothesis,meaning data is non-stationary
 ADF statistic	p-value
 1.7358 	 0.9982 

 # We will test the existence of cointegration by calculating the model residues, and then we will test the residues with the Dickey-Fuller test.

resid = lin_CHN_HAC.resid
adfuller(resid,autolag="BIC) # Based on p-value we can reject null hypothesis,meaning data is stationary
 ADF statistic	p-value
 -6.6527 	 0.0000 

4.2.3. Examination of the functional form of the model

 # The Ramsey RESET (Regression Equation Specification Error Test) is a test designed to test for incorrect model specification. 
 # Incorrect model specification can occur due to:
 - we did not include an explanatory variable in the model,
 - wrong choice of functional model form,
 - inclusion of irrelevant variables in the model.
 # To test the functional form of the model, we use the Ramsey RESET test with formed hypotheses:
    H0: The functional form is well specified
    H1: Functional form is not well specified
 # In case of rejecting the null hypothesis and accepting the alternative hypothesis that the functional form is not well specified,
 # Ramsey test only gives us an indication that the wrong functional form of the model was chosen but the test itself does not offer a solution in terms of which form is suitable for the model. 
 # Adaptation of the functional form of the model is based on theoretical knowledge.

USA

linear_reset(lin_USA, power=2, test_type='fitted', use_f=False, cov_kwargs=None)
 χ2	 p-value
 0.2222  0.6374 
 # Based od p-value we can not reject null hypotheses, meaning that functional form is well specified

Germany

linear_reset(lin_GER, power=2, test_type='fitted', use_f=False, cov_kwargs=None)
 χ2	 p-value
 0.8995  0.3429 
 # Based od p-value we can not reject null hypotheses, meaning that functional form is well specified

China

linear_reset(lin_CHN, power=2, test_type='fitted', use_f=False, cov_kwargs=None)
 χ2	 p-value
 0.1370  0.7113 
 # Based od p-value we can not reject null hypotheses, meaning that functional form is well specified

4.2.4. Testing the assumption of normality

 # The normality of a random term is one of the fundamental assumptions that must be met in order for the least squares method to be applied. 
 # If the assumption of normality is not satisfied, then we say that the model is not the best linear unbiased estimator, therefore it affects t and F statistics, 
 # and thus the statistical significance of parameters and tests.
 # The test on the basis of which we check the normality of the residue is the Jarque-Bera test. The application of the test is based on the calculation of the second, third and fourth central moment of the residual. 
 # The statistic on which the Jarque-Bera test is based is the χ2 probability distribution with two degrees of freedom. The hypotheses that are tested using the test are:
    H0: µ1 = µ2
    H1: µ1 ≠ µ2
 
USA

stats.jarque_bera(resid)
 χ2	 p-value
 2.4729  0.2904 
 # Based on p-value we can not reject null hypotheses, meaning that residual normality assumption is satisfied

Germany

stats.jarque_bera(resid)
 χ2	 p-value
 0.8652  0.6488 
 # Based on p-value we can not reject null hypotheses, meaning that residual normality assumption is satisfied

China

stats.jarque_bera(resid)
 χ2	 p-value
 2.1158  0.3472 
 # Based on p-value we can not reject null hypotheses, meaning that residual normality assumption is satisfied.

4.2.5. Testing the assumption of homoscedasticity

 # The assumption of the existence of homoscedasticity is one of the basic assumptions for the application of the least squares method. 
 # By homoscedasticity he meant that the variance of the random error is the same (constant) for all observations
 # The reasons that can lead to the existence of heteroscedasticity are:
 - the existence of outliers in the data,
 - measurement errors,
 - existence of subpopulations in the data,
 - model specification errors (absence of an important variable, incorrect functional form, etc.).
 # The tests most commonly used to test the homoscedasticity assumption are:
 - Breush-Pagan test
 - White test
 # Given that the sample in this model consists of 38 units and that we have a model with only two explanatory variables, 
 # we will use the White test to verify the existence of homoscedasticity of variance.
 # Hypothesis testing hypothesis testing is formulated as follows:
    H0: δ2 = δ2 = constant
    H1: δ2 ≠ δ2
 # In case of heterosexasticity, the methods we can use to correct the problem are:
 - respecification of the initial model,
 - transformation of initial variables,
 - use of robust standard errors (White correction),
 - using the method of weighted least squares.

USA

white_test = het_white(resid,  lin_USA_HAC.model.exog)
 χ2	 p-value
 6.2298  0.2845 
 # Based on p-value we can not reject null hypotheses, meaning that existence of homoscedasticity is satisfied.

Germany

white_test = het_white(resid,  lin_GER_HAC.model.exog)
 χ2	  p-value
 15.8164  0.0074 
 # Based on p-value we can reject null hypotheses, meaning that we have problem with heterosexasticity.
 # Because we had problem with autocorrelation, we used Newey-West method (HAC) to solve for autocorrelation.
 # HAC will also deal with heterosexasticity. The application of robust standard errors does not change the value of the coefficients of the model, 
 # but corrects the efficiency of t statistics so that drawing conclusions about statistical significance is improved.

China

white_test = het_white(resid,  lin_CHN_HAC.model.exog)
 χ2	 p-value
 3.8145  0.5764 
 # Based on p-value we can not reject null hypotheses, meaning that existence of homoscedasticity is satisfied.

4.2.6. Testing the existence of multicollinearity

 # Multicollinearity tells us about the relationship between independent variables. 
 # High multicollinearity affects the assumption that there is no perfect correlation between independent variables in the model.
 # Some of the ways to solve multicollinearity are:
 - use transformed data,
 - use the first differential variable,
 - change the pattern by adding variables,
 - transform the starting model. 
 # Determination of multicollinearity is performed through the correlation matrix and through the variance growth factor test. 
 # There is a rule that the existence of a correlation between variables greater than 0.8 can affect the occurrence of multicollinearity.

USA

USA.corr()

	      log_Bt.USA   log_RP(USA)	log_DI(USA)
log_Bt.USA    1	           0.006976	0.669405
log_RP(USA)   0.006976	   1	        0.21457
log_DI(USA)   0.669405	   0.21457	1

 # From correlation matrix we wee that highest value of correlation is between variables:
 # "log_Bt.USA " and "log_DI(USA)". This variables are number of tourist and disposable income.
 # The value of correlation is 0.67

USA['Intercept'] = 1
vif = pd.DataFrame()
vif["variables"] = USA.columns
vif["VIF"] = [variance_inflation_factor(USA.values, i) for i in range(USA.shape[1])]
print(vif)
 # To calculate VIF me must include intercept in VIF calcuation.
 variables	VIF
 log_Bt.USA	1.879 
 log_RP(USA)     1.087 
 log_DI(USA)	1.969 
 Intercept	13,880.087 
 # Since we have no VIF higher that 10, we dont have problem with multicollinearity.

Germany

GER.corr()

	     log_Bt.GER	  log_RP(GER)  log_DI(GER)
log_Bt.GER   1	          -0.650068    0.575 
log_RP(GER)  -0.650068	  1	       -0.748 
log_DI(GER)  0.575222     -0.748499    1.000 

 # From correlation matrix we wee that highest value of correlation is between variables:
 # "log_DI(GER) " and "log_RP(GER)". This variables are disposable income and relative prices.
 # The value of correlation is -0.75.

GER['Intercept'] = 1
vif = pd.DataFrame()
vif["variables"] = GER.columns
vif["VIF"] = [variance_inflation_factor(GER.values, i) for i in range(GER.shape[1])]
print(vif)

 variables	VIF
 log_Bt.GER	1.787178
 log_RP(GER)	2.71936
 log_DI(GER)	2.346649
 Intercept	30150.62024
 # Since we have no VIF higher that 10, we dont have problem with multicollinearity.

China

CHN.corr()

	     log_Br.CHN	 log_RP(CHN)  log_DI(CHN)
log_Br.CHN   1	         0.482936     0.888 
log_RP(CHN)  0.482936	 1	      0.681 
log_DI(CHN)  0.88778	 0.680785     1.000 

 # From correlation matrix we wee that highest value of correlation is between variables:
 # "log_Br.CHN " and "log_DI(CHN)". This variables are number of tourist and disposable income.
 # The value of correlation is 0.88

CHN['Intercept'] = 1
vif = pd.DataFrame()
vif["variables"] = CHN.columns
vif["VIF"] = [variance_inflation_factor(CHN.values, i) for i in range(CHN.shape[1])]
print(vif)

 variables	VIF
 log_Br.CHN	5.424343
 log_RP(CHN)	2.141768
 log_DI(CHN)	7.752081
 Intercept	11172.50139

 # Since we have no VIF higher that 10, we dont have problem with multicollinearity.

5. Suggestion of  a valid model

 # Based on the theoretical aspect of the work so far and the tests performed to formulate the model and evaluate the parameters, 
 # we suggest the final appearance of the econometric model of tourism demand in BiH.


          const.     log_RP    log_DI
Country    ß0	      ß1	ß2	 R2     F stat
USA       -33.6593*  -1.6979   4.3826*   0.468  0.000
           (0.000)    (0.187)   (0.000)			

Germany   8.833      -15.3065*  1.6345   0.44   0.000
           (0.388)    (0.000)    (0.218) 

China   -52.0105*    -3.998*    6.5191*  0.816  0.000
         (0.000)      (0.000)    (0.000) 

*, statistically significant at the level of 1%
(.), p-value

const. = it is the value of constant (intercept) of a model
log_RP = it is the variable relative prices
log_DI = it is the variable disposabe 

The mathematical appearance of the model takes the form:

a) US 
log number of tourists = -33.66 + 4.38 log income -1.69 logrelative prices

b) 
Germany log number of tourists = 8.83 + 1.63 log income -15.30 logrelative prices

c) 
China log number of tourists = -52.01 + 6.52 log income -3.99 logrelative prices

6. References

Babić-Hodović, V., Domazet, A., & Kurtović, E. (2012). Basics of marketing (fourth). Sarajevo: Faculty of Economics in Sarajevo.
Botti, L., Peypoch, N., Randriamboarison, R., & Solonandrasana, B. (2006). An Econometric Model of Tourism Demand in France. Anatolia, 19 (1), 41–50. https://doi.org/10.1080/13032917.2008.9687052
Brockwell, P. J., & Davis, R. A. (2002). Introduction to Time Series and Forecasting. Journal of the American Statistical Association (Second). https://doi.org/10.2307/2965440
David E. Allen, G. Y. (2011). Modeling Domestic Tourism Demand in Australia A Dynamic Panel Data Approach. Economic Journal of Emerging Markets, 1 (1), 1–11.
Dritsakis, N., & Athanasiadis, S. (2014). AN ECONOMETRIC MODEL OF TOURIST DEMAND: The Case of Greece, (November 2014), 11. https://doi.org/10.1300/J150v07n02
Dritsakis, N., & Athanasiadis, S. (2000). An Econometric Model of Tourist Demand. Journal of Hospitality & Leisure Marketing, 7 (2), 39–49. https://doi.org/10.1300/J150v07n02_03
Diaz, M. R., & Espino-Rodríguez, T. F. (2016). Determining the sustainability factors and performance of a tourism destination from the stakeholders ’perspective. Sustainability (Switzerland), 8 (9). https://doi.org/10.3390/su8090951
Dževad, Š., & Rahimić, Z. (2009). Management (Second). Sarajevo: Faculty of Economics in Sarajevo.
Garcia, T. (2003). INTRODUCTORY STATISTICS. Computer. https://doi.org/10.1016/B978-0-08-099936-4.00036-X
Gržinić, J. (2019). INTRODUCTION TO TOURISM history, development, perspectives (1st ed.). Pula: Juraj Dobrila University of Pula.
Habibi, F., & Abbasinejad, H. (2011). Dynamic panel data analysis of European tourism demand in Malaysia. Iranian Economic Review, 15 (29), 15.
Hanacek, J. (2010). "Phases of research process". Retrieved from http://www.academia.edu/download/34312110/4Phases_of_research_process_-_Hanacek.doc
Ivanov, V., & Kilian, L. (2005). A practitioner’s guide to lag order selection for VAR impulse response analysis. Studies in Nonlinear Dynamics and Econometrics, 9 (1), 36. https://doi.org/10.2202/1558-3708.1219
King, B., & Ringer, G. (2009). The Advanced Econometrics of Tourism Demand Routledge Advances in Tourism. https://doi.org/10.4324/9780203891469
Liew, V. K. (2004). On Autoregressive Order Selection Criteria. Computational Economics, (March), 1–14. Retrieved from http://129.3.20.41/eps/comp/papers/0404/0404001.pdf
Lukic, R. (2012). The effects of cross-border tourist purchases on retail performance. Singidunum Journal of Applied Sciences (Vol. 9). https://doi.org/10.5937/sjas1202021l
Njegovan, Z. (2016). Economics of tourism and rural tourism. (Full Professor Dr. Nedeljko Tica, Ed.) (1st ed.). Novi Sad: University of Novi Sad, Faculty of Agriculture, Novi Sad.
Page, S. J. (2009). Tourism Management (Third edit). Elsevier Ltd.
Resić, E. (2006). Collection of tasks from Statistics.
Resić, P. dr. E. (2017). Nonparametric statistical tests.
Silva, J. da. (2016). Tourism Management, 11–73.
Somun-Kapetanović, R. (2012). Statistics in Economics and Management (Third). Sarajevo: Faculty of Economics in Sarajevo.
Song, H., Witt F., S., & Li, G. (2009). The Advanced Econometrics of Tourism Demand. Routledge 270 Madison Ave, New York, NY 10016.
Šošić, I. (2004). Applied statistics.
Tong, L., Ying, C., & Qian, W. (2018). ADVANCED ECONOMETRICS. Advances in Intelligent Systems and Computing (Vol. 691). https://doi.org/10.1007/978-3-319-70990-1_14
UN World Tourism Organization. (2018). 2018 Edition UNWTO Tourism Highlights. https://doi.org/DOI: https://doi.org/10.18111/9789284419876
Wang, L., Zhang, H., & Li, W. (2012). Analysis of causality between tourism and economic growth based on computational econometrics. Journal of Computers, 7 (9), 2152–2159. https://doi.org/10.4304/jcp.7.9.2152-2159
Vesna Babić-Hodović, Anto Domazet, E. K. (2006). Basics of marketing. Faculty of Economics in Sarajevo.
Wei, W. W. S. (2012). Time Series Anylysis Univariate and multivariate methods. (D. Lynch, Ed.), Bayesian Econometric Methods (Second). Greg Tobin. https://doi.org/10.1017/cbo9780511802447.020
Williams, R. (2015). Heteroskedasticity.
Wooldridge, J. M. (2013). Introductory Econometrics: A Modern Approach, Fifth Edition. (Joe Sabatino, Ed.).
Yap, G. C. L. (2010). AN ECONOMETRIC ANALYSIS OF AUSTRALIAN DOMESTIC TOURISM DEMAND.

Web:
 http://www.worldbank.org
https://tradingeconomics.com/
https://www.statista.com/
https://www.indexmundi.com
https://www.ceicdata.com/en
https://www.bls.gov
https://www.oecd.org
https://www.destatis.de/EN/Homepage.html
http://www.stats.gov.cn/english/
http://www.bhas.ba
https://www.statisticssolutions.com/normality/








